{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(461, 64, 7196)\n",
      "Number of NaN values in the array: 0\n"
     ]
    }
   ],
   "source": [
    "\"Check data\"\n",
    "import numpy as np \n",
    "\n",
    "data = np.load(\"C:\\\\Users\\\\msi\\\\Desktop\\\\Constanze\\\\Docs\\\\DATA\\\\PREPROCESSED\\\\S01\\\\data.npy\")\n",
    "print(data.shape)\n",
    "nan_count = np.isnan(data).sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of NaN values in the array: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found sessions: 11\n",
      "Session S01: Max trial length = 7196\n",
      "Session S03: Max trial length = 7593\n",
      "Session S04: Max trial length = 6545\n",
      "Session S05: Max trial length = 7744\n",
      "Session S06: Max trial length = 7062\n",
      "Session S07: Max trial length = 6748\n",
      "Session S08: Max trial length = 14997\n",
      "Session S09: Max trial length = 6515\n",
      "Session S10: Max trial length = 6161\n",
      "Session S11: Max trial length = 6894\n",
      "Session S12: Max trial length = 6479\n",
      "Overall MAX TRIAL LENGTH: 14997\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Define data directory as a Path object\n",
    "data_dir = Path(\"C:\\\\Users\\\\msi\\\\Desktop\\\\Constanze\\\\Docs\\\\DATA\\\\PREPROCESSED\\\\\")\n",
    "\n",
    "# Find all sessions matching the pattern \"S*\"\n",
    "trials = sorted(data_dir.glob(\"S*\"))\n",
    "\n",
    "print(f\"Found sessions: {len(trials)}\")\n",
    "\n",
    "# Initialize variables\n",
    "train = True\n",
    "max_trial_length = 0  # Track maximum trial length across sessions\n",
    "\n",
    "# Iterate over all sessions\n",
    "for file_path in trials:\n",
    "    data_path = file_path / \"data.npy\"\n",
    "    _labels_path = file_path / \"labels.npy\"\n",
    "\n",
    "    # Ensure the files exist before processing\n",
    "    if not data_path.exists() or not _labels_path.exists():\n",
    "        print(f\"Skipping session {file_path.name}: Missing data or labels file.\")\n",
    "        continue\n",
    "\n",
    "    # Load labels\n",
    "    _labels = np.load(_labels_path, allow_pickle=True)\n",
    "\n",
    "    # Perform train/validation split\n",
    "    if train:\n",
    "        selection = np.concatenate(\n",
    "            [np.argwhere(_labels == label_id).flatten()[:-5] for label_id in np.unique(_labels)]\n",
    "        )\n",
    "    else:\n",
    "        selection = np.concatenate(\n",
    "            [np.argwhere(_labels == label_id).flatten()[-5:] for label_id in np.unique(_labels)]\n",
    "        )\n",
    "\n",
    "    # Load selected data\n",
    "    selected_data = np.load(data_path, allow_pickle=True)[selection]\n",
    "\n",
    "    # Update max trial length\n",
    "    session_max_length = selected_data.shape[2]  # Assuming shape is (n_trials, n_channels, n_samples)\n",
    "    max_trial_length = max(max_trial_length, session_max_length)\n",
    "\n",
    "    print(f\"Session {file_path.name}: Max trial length = {session_max_length}\")\n",
    "\n",
    "# Print overall max trial length\n",
    "print(\"Overall MAX TRIAL LENGTH:\", max_trial_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as JSON at: C:\\Users\\msi\\Desktop\\Constanze\\Docs\\DATA\\marker\\textmaps\\S01.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "session = \"S01\"\n",
    "marker = pd.read_csv(f\"C:\\\\Users\\\\msi\\\\Desktop\\\\Constanze\\\\Docs\\\\DATA\\\\marker\\\\{session}.csv\")\n",
    "textmaps = marker.iloc[1:-1,0:2]\n",
    "\n",
    "# Save the DataFrame as a JSON file\n",
    "output_path = f\"C:\\\\Users\\\\msi\\\\Desktop\\\\Constanze\\\\Docs\\\\DATA\\\\marker\\\\textmaps\\\\{session}.json\"\n",
    "textmaps.to_json(output_path, orient='records', lines=True)\n",
    "\n",
    "print(f\"Data saved as JSON at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed JSON saved to: C:\\Users\\msi\\Desktop\\Constanze\\Docs\\DATA\\marker\\textmaps\\S01_transformed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load your JSON file\n",
    "input_path = \"C:\\\\Users\\\\msi\\\\Desktop\\\\Constanze\\\\Docs\\\\DATA\\\\marker\\\\textmaps\\\\S01.json\"\n",
    "output_path = \"C:\\\\Users\\\\msi\\\\Desktop\\\\Constanze\\\\Docs\\\\DATA\\\\marker\\\\textmaps\\\\S01_transformed.json\"\n",
    "\n",
    "with open(input_path, \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Transform into the desired format\n",
    "transformed_data = {entry[\"sentences\"]: int(entry[\"labels\"]) for entry in data}\n",
    "\n",
    "# Save the transformed data\n",
    "with open(output_path, \"w\") as file:\n",
    "    json.dump(transformed_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Transformed JSON saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
